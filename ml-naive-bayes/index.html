<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="水远，怎知流水外，却是乱山尤远。"><title>【机器学习笔记】3. Naive Bayes &amp; Logistic Regression | 山尤远</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">【机器学习笔记】3. Naive Bayes &amp; Logistic Regression</h1><a id="logo" href="/.">山尤远</a><p class="description">THY’s Blog</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">【机器学习笔记】3. Naive Bayes &amp; Logistic Regression</h1><div class="post-meta">Oct 26, 2019<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.9k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 9</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#分类中的朴素贝叶斯方法（Naive-Bayes-Classifier"><span class="toc-text">分类中的朴素贝叶斯方法（Naive Bayes Classifier)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#目标"><span class="toc-text">目标</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#基本假设"><span class="toc-text">基本假设</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#理论依据（贝叶斯定理）"><span class="toc-text">理论依据（贝叶斯定理）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#朴素贝叶斯分类器训练（Training-Naive-Bayes-Classifier）"><span class="toc-text">朴素贝叶斯分类器训练（Training Naive Bayes Classifier）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#朴素贝叶斯分类器测试（Testing-Naive-Bayes-Classifier）"><span class="toc-text">朴素贝叶斯分类器测试（Testing Naive Bayes Classifier）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#算法性能的衡量指标"><span class="toc-text">算法性能的衡量指标</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#逻辑斯谛回归（Logistic-Regression"><span class="toc-text">逻辑斯谛回归（Logistic Regression)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#目标："><span class="toc-text">目标：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#基本假设："><span class="toc-text">基本假设：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#理论依据："><span class="toc-text">理论依据：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#实际问题：数据的不平衡性"><span class="toc-text">实际问题：数据的不平衡性</span></a></li></ol></li></ol></div></div><div class="post-content"><blockquote>
<p>整理自同学的笔记。<br>监督学习是指有目标变量或预测目标的机器学习方法，包括分类和回归。</p>
</blockquote>
<h3 id="分类中的朴素贝叶斯方法（Naive-Bayes-Classifier"><a href="#分类中的朴素贝叶斯方法（Naive-Bayes-Classifier" class="headerlink" title="分类中的朴素贝叶斯方法（Naive Bayes Classifier)"></a>分类中的朴素贝叶斯方法（Naive Bayes Classifier)</h3><p>以垃圾邮件的分类（Span Detecor）为例。</p>
<h4 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h4><p>对于训练过的模型，给定${\bf x}$，给出$P(spam|{\bf x})$。</p>
<p>训练数据记作$ \{ \mathbf{x} _ i,   y _ i \} $， $y_i \in {\mathcal C} = \{spam, not \_ spam \} $。</p>
<p>eg: spam email: laptop with the lowest price.</p>
<h4 id="基本假设"><a href="#基本假设" class="headerlink" title="基本假设"></a>基本假设</h4><ol>
<li><p>属性值$x_i$条件独立于标签值，即<br>$$<br>P(x_1, x_2, \cdots ,x_{| \cal{X} |}|{\mathcal C}) = \prod_i P(x_i | {\mathcal C })<br>$$</p>
<p>以垃圾邮件分类为例，该问题中的样本${\bf x}_i$ 为表征邮件属性的矢量（比如词向量），表示邮件的整体特征。如果不考虑这一假设，在通常的采样中对$P({\bf x}|c)$的估计往往会导出很小的值（不容易找到两封一样的邮件）。</p>
<p>而这一假设为我们带来的好处则是摆脱了属性捆绑的桎梏，将单个属性作为统计与概率估计的原子单位，既提高了对数据的利用率也有效地降低了模型需要的参数数目。当然这以真实性为代价。</p>
</li>
<li><p>属性值的分布独立于其出现的位置：</p>
<p>$$<br>P(x_i = w_k|c) = P(x_j=w_k|c),\forall i\not=j<br>$$</p>
<p>亦即：</p>
<p>$$<br>P(x_i = w_k|c) = P(w_k|c),\forall i<br>$$</p>
<p>这一条件是我们脱离了对邮件长度与位置的依赖，估计中我们就只需要考虑词频，进一步降低了估计参数的数目和复杂度。</p>
</li>
</ol>
<h4 id="理论依据（贝叶斯定理）"><a href="#理论依据（贝叶斯定理）" class="headerlink" title="理论依据（贝叶斯定理）"></a>理论依据（贝叶斯定理）</h4><p>$$<br>\begin{align}<br>{\hat y} &amp;= \arg \max _ {c \in {\mathcal C} } P(c | {\bf x} ) \\<br>&amp;= \arg \max _ {c \in {\mathcal C} } \frac{P({\bf x}|c) P(c) } {P({\bf x} ) } \\<br>&amp;= \arg \max _ {c \in {\mathcal C} } P({\bf x}|c)P(c) \\<br>&amp;= \arg \max _ {c \in {\mathcal C} } P(c) \prod_i P(x_i | c)  \ (assumption \ 1)  \\<br>&amp;= \arg \max _ {c \in {\mathcal C} } P(c) \prod_k P(w_k|c) ^ {t _ k} \ (assumption \ 2) \\<br>\end{align}<br>$$</p>
<p>其中的$P(c)$为先验概率，从采样数据中估计。使先验概率更接近真实分布这一点对采样的多样性提出了一定的要求。</p>
<p>最后的$P(w _ k|c)$可以用表示$P(w_k|c) = \dfrac{n_{ck}}{n_c}$,其中$n_c=\sum_{i : y=c} | x _ i| $表示c类出现的次数，$n_{ck}$表示c类中词$w_k$出现的次数。但是注意到如果在采样中只要有$n_{c k}=0$,那在估计中就一定会有$P(w_k|c)=0$,这在实际中并不是合理的。为了解决这种问题，有一种方案是Laplace Smoothing:</p>
<p>$$<br>P(w_k|c) = \frac{n_{c k} + 1} {n _ c + | \mathcal{V} | }<br>$$</p>
<h4 id="朴素贝叶斯分类器训练（Training-Naive-Bayes-Classifier）"><a href="#朴素贝叶斯分类器训练（Training-Naive-Bayes-Classifier）" class="headerlink" title="朴素贝叶斯分类器训练（Training Naive Bayes Classifier）"></a>朴素贝叶斯分类器训练（Training Naive Bayes Classifier）</h4><p>$$<br>\begin{align}<br>&amp;\text{Input: trainning samples } {\mathcal D} = \{ ({\bf x_i},y_i) \} \\<br>&amp; {\mathcal V} \leftarrow \text{the set of distinct words and other tokens in }  {\mathcal D} \\<br>&amp; \text{for each target value } c \in {\mathcal C}, \text{ do} \\<br>&amp; ~ ~ ~ ~ ~ ~ ~ ~  {\mathcal D_c} \leftarrow \text{the training samples whose labels are c} \\<br>&amp; ~ ~ ~ ~ ~ ~ ~ ~ P(c) \leftarrow \dfrac{|{\mathcal D_c}|}{|{\mathcal D}|} \\<br>&amp; ~ ~ ~ ~ ~ ~ ~ ~ T_c \leftarrow \text{a single document by concentrating all training samples in } \mathcal{D} _ c \\<br>&amp; ~ ~ ~ ~ ~ ~ ~ ~ n_c \leftarrow |T_c| \\<br>&amp; ~ ~ ~ ~ ~ ~ ~ ~ \text{for } w_k \in \cal{V} \text{ do} \\<br>&amp;  ~ ~ ~ ~ ~ ~ ~ ~  ~ ~ ~ ~ ~ ~ ~ ~ n_{ck} \leftarrow  \text{the number of times the word } w_k  \text{ occurs in } T_c \\<br>&amp;  ~ ~ ~ ~ ~ ~ ~ ~  ~ ~ ~ ~ ~ ~ ~ ~ P(w_k|c) = \dfrac{n _ {ck} + 1}{n_c+ | \mathcal{V} | } \\<br>&amp;  ~ ~ ~ ~ ~ ~ ~ ~ \text{endfor} \\<br>&amp; \text{endfor}<br>\end{align}<br>$$</p>
<p>所谓训练，就是计算$P(w_k|c)$的表罢了。</p>
<h4 id="朴素贝叶斯分类器测试（Testing-Naive-Bayes-Classifier）"><a href="#朴素贝叶斯分类器测试（Testing-Naive-Bayes-Classifier）" class="headerlink" title="朴素贝叶斯分类器测试（Testing Naive Bayes Classifier）"></a>朴素贝叶斯分类器测试（Testing Naive Bayes Classifier）</h4><p>$$<br>\begin{align}<br>&amp; \text{Input: A new sample } {\bf x}, \text{ 设} x_i \text{是} {\bf x}  \text{的第 i 个属性}, I = \emptyset \\<br>&amp; \text{for } x_1, \cdots, x_i \text{ do} \\<br>&amp; ~ ~ ~ ~ ~ ~ ~ ~ \text{if } \exists w_k \in \mathcal{V} \text{ such that } w_k = x_i, \text{ then} \\<br>&amp; ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ I \leftarrow I \cup k \\<br>&amp; ~ ~ ~ ~ ~ ~ ~ ~ \text{end if} \\<br>&amp; \text{end for} \\<br>&amp; \text{predict the label of } \mathbf{x} \text{ by } \hat y= \arg \max_{c \in {\mathcal C}}P(c) \prod _ { i \in I} P(w_i|c)<br>\end{align}<br>$$</p>
<p>这个算法虽然简单，但是好用。</p>
<h4 id="算法性能的衡量指标"><a href="#算法性能的衡量指标" class="headerlink" title="算法性能的衡量指标"></a>算法性能的衡量指标</h4><ol>
<li><p>准确率（Accuracy)<br>$$<br>\text{Accuracy} = \frac{\text{ # correctly predicted samples} } {\text{ # total samples} }<br>$$<br>这个指标并不适用于一般情景，它忽略了两种分类错误的不同风险。</p>
</li>
<li><p>查准率（Precision)、召回率（Recall）、F-score：</p>
<table>
<thead>
<tr>
<th></th>
<th>T（正确）</th>
<th>F（错误）</th>
<th>总计</th>
</tr>
</thead>
<tbody><tr>
<td>P（正例）</td>
<td>TP</td>
<td>FP（第一类错误， 假正例）</td>
<td>正例总数</td>
</tr>
<tr>
<td>N（反例）</td>
<td>TN</td>
<td>FN（第二类错误， 假反例）</td>
<td>反例总数</td>
</tr>
<tr>
<td>总计</td>
<td>预测正确总数</td>
<td>预测错误总数</td>
<td>样例总数</td>
</tr>
</tbody></table>
<p>则<br>$$<br>\begin{align}<br>&amp; \text{Precision} = \frac{TP}{TP+FP} \\<br>&amp; \text{Recall} = \frac{TP}{TP+FN}  \\<br>&amp; F_1 = \dfrac{2}{\dfrac{1}{\text{Precision} } + \dfrac{1}{\text{Recall} } }<br>\end{align}<br>$$</p>
</li>
</ol>
<h3 id="逻辑斯谛回归（Logistic-Regression"><a href="#逻辑斯谛回归（Logistic-Regression" class="headerlink" title="逻辑斯谛回归（Logistic Regression)"></a>逻辑斯谛回归（Logistic Regression)</h3><h4 id="目标："><a href="#目标：" class="headerlink" title="目标："></a>目标：</h4><p>给定集合$ \{ ({\bf x}_i, y_i \} ^ n _ {i=1} $, 其中$y_i \in \{0,1\}$,寻找映射：<br>$$<br>f:X\rightarrow Y, where\ X=(X_1,\cdots,X_d)\ and\ Y\in\{0,1\}<br>$$</p>
<h4 id="基本假设："><a href="#基本假设：" class="headerlink" title="基本假设："></a>基本假设：</h4><ol>
<li>$Y \sim Bern(P)$, $Y$ 服从伯努利二项分布，$P(Y=1) = p$.</li>
<li>$X = (X_1,\cdots,X_d)$中的$X_j$是连续随机变量。</li>
<li>高斯分布: $P(X_j|Y=0)\sim N(\mu _ {j0}, \sigma _ j^2),P(X_j|Y=1)\sim  N(\mu _ {j1},\sigma_j^2)$</li>
<li>$X_i, X_j$条件独立于$Y$, $\forall i\not=j$.</li>
</ol>
<h4 id="理论依据："><a href="#理论依据：" class="headerlink" title="理论依据："></a>理论依据：</h4><p>综上,</p>
<p>$$<br>\begin{align}<br>P(Y=0|X) &amp;= \dfrac{P(X|Y=0)P(Y=0)}{P(X|Y=0)P(Y=0)+P(X|Y=1)P(Y=1) }<br>\\ &amp;= \dfrac{1}{1+\dfrac{P(X|Y=1)P(Y=1)}{P(X|Y=0)P(Y=0)} }<br>\\ &amp;= \dfrac{1}{1+\exp (\ln (\dfrac{P(X|Y=1)P(Y=1)}{P(X|Y=0)P(Y=0) } ) ) }<br>\\ &amp;= \dfrac{1}{1+ \exp(\sum_j \ln (\dfrac{P(X_j|Y=1)}{P(X_j|Y=0)})+ \ln \dfrac{p}{1-p})} (assumption\ 4)<br>\end{align}<br>$$</p>
<p>而</p>
<p>$$<br>\begin{align}<br>\sum_j \ln (\frac{P(X_j|Y=1)}{P(X_j|Y=0)}) &amp;= \sum_j \ln (\frac{\exp (-\dfrac{(X_j-\mu_{j1}) ^ 2}{2\sigma_j ^ 2})}{\exp (-\dfrac{(X_j-\mu_{j0}) ^ 2}{2 \sigma_j^2})})(assumption\ 3)<br>\\ &amp;= \sum_j\dfrac{\mu_{j1}-\mu_{j0}}{\sigma_j^2}X_j+\sum_j\frac{\mu_{j0}^2-\mu_{j1}^2}{2\sigma_j^2}<br>\end{align}<br>$$<br>将其带回原式，<br>$$<br>\begin{align}<br>P(Y=0|X) &amp;= \frac{1}{1+ \exp (\sum_j \dfrac{\mu_{j1}-\mu_{j0} }{\sigma_ j^ 2} X_j + \sum_j \dfrac{\mu_{j0}^ 2- \mu_ {j1}^ 2} {2 \sigma_ j^ 2}+ \ln \dfrac{p}{1-p})}<br>\\  &amp;= \frac{1}{1+ \exp (\sum_j w_j X_j + w_0)}<br>\end{align}<br>$$<br>于是又有<br>$$<br>\begin{align}<br>P(Y=1|X) &amp;= \frac{\exp(\sum_jw_jX_j+w_0)}{1+ \exp(\sum_jw_jX_j+w_0)}<br>\end{align}<br>$$<br>可见决策平面$\sum_jw_jX_j+w_0=0$是线性的。当找到决策平面时，该分类问题就会迎刃而解。而下一步，我们就需要找出需要的权向量${\bf w}$。</p>
<p><strong>采用最大似然估计法：</strong><br>$$<br>\begin{align}<br>\hat {\bf w} &amp;= \arg \max_ \mathbf{w} \prod_i P(y_i|X_i,{\bf w})<br>\\ &amp;= \arg \max_ \mathbf{w} \sum_i \ln (P(y_i|X_i, {\bf w}) )<br>\end{align}<br>$$</p>
<p>令 $-L({\bf w}) = \sum_i(y_i \ln(P(Y=1|X_i,{\bf w}))+(1-y_i) \ln(P(Y=0|X_i,{\bf w})))$,则问题转化为：<br>$$<br>\hat{\bf w} = \arg \min_{\bf w}L({\bf w})<br>$$<br>那么似乎可以用梯度下降法来求解该问题。（解的存在性、唯一性（严格凸、强凸））</p>
<p>采用正则化可以保证这两点：<br>$$<br>\hat {\bf w} = \arg \max_{\bf w} L({\bf w})+\frac{\lambda}{2}\Vert{\bf w}\Vert_2^2<br>$$</p>
<p>对于多分类问题，可以训练多个分类器。其中$Y\in \cal{C} = {c_1, \cdots, c_k}$，可令</p>
<p>$$<br>\begin{align}<br>P(Y\not=c_k|X) &amp;= \frac{1}{1+ \exp(\sum_jw_{kj}X_j+w_{k0})}<br>\end{align}<br>$$</p>
<p>$$<br>P(Y=c_k|X) =<br>\begin{cases}<br>&amp;  \dfrac{\exp(\sum_j w_{kj}X_j + w_{k0})}{1 + \sum_{k=1}^{K-1}\exp(\sum_j w_{kj}X_j + w_{k0}) } &amp; k = 1, \cdots,K-1 \\<br>&amp; \dfrac{1}{1 + \sum_{k=1}^{K-1}\exp(\sum_j w_{kj}X_j + w_{k0}) } &amp; k = K-1<br>\end{cases}<br>$$</p>
<h4 id="实际问题：数据的不平衡性"><a href="#实际问题：数据的不平衡性" class="headerlink" title="实际问题：数据的不平衡性"></a>实际问题：数据的不平衡性</h4><p>来自不同分类的数据数目不平衡时，回导致训练得出的决策平面有更大的偏移。</p>
<p>解决方案包括：</p>
<ul>
<li><p>undersample（主要）</p>
</li>
<li><p>oversample</p>
</li>
</ul>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><iframe src="/donate/?AliPayQR=/img/AliPayQR.jpg&amp;WeChatQR=/img/WeChatQR.png&amp;GitHub=null&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="http://hytu99.github.io/ml-naive-bayes/" data-id="ck7n3bzcz000fqku71qak58ns" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAAAAACKZ2kyAAABxklEQVR42u3aQY7DIAwF0N7/0pltpTbo2wSSjh6rqKnCg4WFbV6veBxv4/2Xz7ef/xx/87Vi4OLiTnOP4RhzzxZw1YK/fBMXF3cjdxxoqs8JIlkqLi7ur3DPAlN+fMHFxf2v3LNQlSc2uLi4z+fmR5DkbV7NWJir4eLiTnDzKuW65yX1XVxc3Bb3KI48MI0PQM3ZcXFxt3Cricp44upzdV5cXNyd3PHESXu1MEGr4Vo4eeHi4k5zr0pLqqWQfDFfTmS4uLhbuPPXrZJglJdHTw24uLhbuHl6k4OqB5fmJQ9cXNxl3JyYB7heczRaAC4u7hZur4IyLpheteCovouLi3sTt9oCyRstvXIqLi7uTm7+oWpZJA9S0Qbh4uLexL3qklY1BYqaNLi4uNu5vSZKEryqWxAFMlxc3MXc6lWtPLGZJ+Li4j6Bm7OqTdaZa164uLh7uEdxzLRbem0bXFzc/dxqeSIpg+ZNlHWXOXBxcee5vcuX1XZIbwuigikuLu4ybjUMVXOsXiu3UDDFxcW9lTsTyOYvfuHi4j6TW6jFBmnPZZ0fXFzcBdxqiEmCzkxJFBcX917uTOOz2oDJN+iC+i4uLm6H+wevuMNCHGe66wAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/机器学习/">机器学习</a><a href="/tags/学习笔记/">学习笔记</a></div><div class="post-nav"><a class="pre" href="/ml-svm/">【机器学习笔记】4. Support Vector Machine &amp; Lagrange Duality</a><a class="next" href="/docker/">Docker入门总结</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC80ODk5NS8yNTQ4OQ=="><script>(function(d, s) {
   var j, e = d.getElementsByTagName(s)[0];
   if (typeof LivereTower === 'function') { return; }
   j = d.createElement(s);
   j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
   j.async = true;
   e.parentNode.insertBefore(j, e);
})(document, 'script');
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Paper-Reading/">Paper Reading</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术总结/">技术总结</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/程序设计/">程序设计</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/随笔/" style="font-size: 15px;">随笔</a> <a href="/tags/机试/" style="font-size: 15px;">机试</a> <a href="/tags/编程/" style="font-size: 15px;">编程</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/code-search/" style="font-size: 15px;">code search</a> <a href="/tags/保研/" style="font-size: 15px;">保研</a> <a href="/tags/nlp/" style="font-size: 15px;">nlp</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/学习笔记/" style="font-size: 15px;">学习笔记</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/MSRA/" style="font-size: 15px;">MSRA</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/msra-record/">来MSRA的七个月，我都做了些什么？</a></li><li class="post-list-item"><a class="post-list-link" href="/revise-theme/">Maupassant主题增加访问统计</a></li><li class="post-list-item"><a class="post-list-link" href="/ml-svm/">【机器学习笔记】4. Support Vector Machine & Lagrange Duality</a></li><li class="post-list-item"><a class="post-list-link" href="/ml-naive-bayes/">【机器学习笔记】3. Naive Bayes & Logistic Regression</a></li><li class="post-list-item"><a class="post-list-link" href="/docker/">Docker入门总结</a></li><li class="post-list-item"><a class="post-list-link" href="/ml-gradient-descent/">【机器学习笔记】2. Gradient Descent</a></li><li class="post-list-item"><a class="post-list-link" href="/ml-linear-regression/">【机器学习笔记】1. Linear Regression</a></li><li class="post-list-item"><a class="post-list-link" href="/deep-code-search/">【论文阅读】Deep Code Search</a></li><li class="post-list-item"><a class="post-list-link" href="/code-template/">机试模板整理</a></li><li class="post-list-item"><a class="post-list-link" href="/first-article/">终于弄好博客啦</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">山尤远.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a><span style="font-size:14px;" id="busuanzi_container_site_pv"><br> 本站总访问量  <span rel="nofollow" id="busuanzi_value_site_pv"></span> 人次,</span><span style="font-size:14px;" id="busuanzi_container_site_uv"> 访客数  <span rel="nofollow" id="busuanzi_value_site_uv"></span> 人.</span></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>