---
title: 【机器学习笔记】4. Support Vector Machine
top: false
cover: false
toc: true
mathjax: true
date: 2019-12-02 01:09:59
password:
summary:
tags:
- 机器学习
- 学习笔记
categories: 机器学习
---

> 整理自同学的笔记

### Support Vector Machine

Training data: $ \\{ (\mathbf{x}_i, y ) \\} _ {i=1} ^ n$,  $y_i \in \mathcal{C} = \\{ -1, 1 \\}$.

Aim: $f(\mathbf{x}, \mathbf{w}, b) = b + \sum_{j=1}^d w_j x_j$, s.t. $y_i = \text{sign}(f(\mathbf{x}_i, w, b))$

#### SVM for linear separable data

**Definition:** A training sample is linear separate if there exists $(\hat{\mathbf{w}}, \hat{b})$, s.t. $y_i = \text{sign}  (f(\mathbf{x}_i, \hat{\mathbf{w}}, \hat{b}))$, $\forall i \in [n] = \\{1, 2, \cdots, n \\}$, which is equivalent to $y_i f(\mathbf{x}_i, \hat{\mathbf{w}}, \hat{b}) > 0$, $\forall i \in [n]$.

<img src="https://pic3.zhimg.com/v2-197913c461c1953c30b804b4a7eddfcc_1200x500.jpg" width = "400" height = "400" alt="SVM示意图" align=center />

点$\mathbf{x}_i$到线$<w, \mathbf{x}> + b = 0$的距离$d(\mathbf{x}_i;\mathbf{w},b) = \dfrac{y_i(\langle \mathbf{w}, \mathbf{x}_i \rangle + b)}{\Vert \mathbf{w} \Vert_2}$.

$$
\max \limits_{w, b} \min \limits_{\mathbf{x}_i \in D} margin(\mathbf{w}, b, D) = \max \limits_{w, b} \min \limits_{\mathbf{x}_i \in D} d(\mathbf{x}_i) = \max \limits_{w, b} \min \limits_{\mathbf{x}_i \in D}  \dfrac{y_i( \langle\mathbf{w}, \mathbf{x}_i \rangle+ b)}{\Vert \mathbf{w} \Vert_2}
$$
**Assumption 1:** Training sample $D = \\{ (\mathbf{x}_i, y_i) \\} $, is linear separable.

**Definition:** 

The geometric margin $\gamma_f (\mathbf{z})$ of a linear classifier $f({\bf x},{\bf w}, b) = \langle {\bf w}, {\bf x} \rangle + b$ at a point $\mathbf{z}$ is its sigmoid Euclidean Distance to the hyperplane $ \\{ \mathbf{x} |  \langle {\bf w}, {\bf x} \rangle + b = 0\\}$.
$$
\gamma_f (\mathbf{z})= \dfrac{y_i(\langle \mathbf{w}, \mathbf{z}_i \rangle + b)}{\Vert \mathbf{w} \Vert_2}
$$

The geometric margin $\gamma_f$ of a linear classifier $f$ for sample $S = \\{ {\bf x}_1, \cdots, {\bf x_n} \\}$ is the minimum margin over the points in the sample. 

$$
\gamma_f = \min \limits_{i \in [n]} \gamma_f ({\bf x}_i)
$$

#### Maximum Margin Classifier

$$
\max \limits_ { {\bf w}, b} \gamma_f = \max \limits_ { {\bf w}, b} \left \\{ \frac{1}{\Vert \mathbf{w} \Vert} \min \limits_{i \in [n] } y _ i( \langle \mathbf{w}, \mathbf{x}_i \rangle + b) \right \\}
$$

即

$$
\max \limits _ { {\bf w}, b} \frac{1}{\Vert {\bf w} \Vert},  \\\\
\text{s.t. } \min \limits _ {i \in [n]} y _ i( \langle \mathbf{w}, \mathbf{x} _ i \rangle + b) = 1  \\\\ 
\Rightarrow y _ i( \langle \mathbf{w} , \mathbf{x} _ i \rangle + b) \ge 1 \\\\ 
\Rightarrow \min \limits_{ {\bf w}, b} \frac{1}{2} \Vert {\bf w} \Vert ^ 2
$$

用反证法可证等号可以取到。

**Definition:** Given a SVM classifier $\langle \mathbf{w}, \mathbf{x}_i \rangle + b = 0$, the marginal hyperplanes are determined by $\vert \langle \mathbf{w}, \mathbf{x}_i \rangle + b \vert  = 1$. The support vectors are the data instance on the marginal hyperplanes. （ i.e. \\{  {\bf x}_i | \langle \mathbf{w}, \mathbf{x}_i \rangle + b \vert  = 1 , {\bf x}_i \in S \\} ）

#### Not separable

minimize $\frac{1}{2} \Vert {\bf w} \Vert ^ 2 + C(training \ errors)$

minimize  $\frac{1}{2} \Vert {\bf w} \Vert ^ 2 + C(distance \ of\ the \ error \  points \ and \ its \ correct \ position)$

SVM for non-separate cases:

$$
\min \limits _ { {\bf w}, b, \epsilon} \frac{1}{2} \Vert \mathbf{w} \Vert + C \sum _ {i=1} ^ {n} \epsilon_i, \\\\
\text{s.t. } y_i (\langle \mathbf{w}, \mathbf{x}_i \rangle + b) \ge 1 - \epsilon_i, \\\\  \epsilon_i \ge 0, i \in [n]
$$
