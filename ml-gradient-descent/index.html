<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="水远，怎知流水外，却是乱山尤远。"><title>【机器学习笔记】2. Gradient Descent | 山尤远</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">【机器学习笔记】2. Gradient Descent</h1><a id="logo" href="/.">山尤远</a><p class="description">THY’s Blog</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">【机器学习笔记】2. Gradient Descent</h1><div class="post-meta">Oct 24, 2019<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 1.9k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 11</span><span class="post-meta-item-text"> 分钟</span></span></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Gradient-Descent"><span class="toc-text">Gradient Descent</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Part-1"><span class="toc-text">Part 1</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Part-2"><span class="toc-text">Part 2</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Algorithm-Gradient-Descent"><span class="toc-text">Algorithm:  Gradient Descent</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Convergence-Rate"><span class="toc-text">Convergence Rate</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#Efficiency-and-limitations"><span class="toc-text">Efficiency and limitations</span></a></li></ol></li></ol></li></ol></div></div><div class="post-content"><blockquote>
<p>整理自同学的笔记</p>
</blockquote>
<h3 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h3><h4 id="Part-1"><a href="#Part-1" class="headerlink" title="Part 1"></a>Part 1</h4><p>$$<br>\min f(\mathbf{x})<br>$$</p>
<ul>
<li><p>$f(\mathbf{x})$ is continuously differentiable.</p>
</li>
<li><p>$f(\mathbf{x})$ is convex.</p>
</li>
</ul>
<hr>
<p><strong>Definition:</strong> A set $C$ is convex if the line segment between any two points in $C$ lies in $C$. that is $\forall x_1,x_2 \in C$, and $\forall \theta \in [0, 1]$, we have $\theta x_1 + (1-\theta)x_2 \in C$.</p>
<p><strong>Definition:</strong> A function $f: \mathbb{R}^n\rightarrow\mathbb{R}$ is convex if $\rm{dom}$ $f$ is convex and if $\mathbf{x},\mathbf{y} \in$ ${\rm dom}f$ and $\theta \in [0, 1]$, we have $f(\theta \mathbf{x} + (1-\theta)\mathbf{y}) \le \theta f(\mathbf{x}) + (1-\theta)f(\mathbf{y})  $.</p>
<p><strong>Definition:</strong> A function is strict convex if strict inequality holds where $\mathbf{x} \ne \mathbf{y}$ and  $\theta \in [0, 1]$.</p>
<p><strong>Definition:</strong> A function is strongly convex with parameter $u$ if  $f - \dfrac{u}{2} \Vert \mathbf{x} \Vert ^2$ is convex.</p>
<hr>
<p><strong>Theorem 1:</strong> Suppose that $f$ is continuously differentiable. Then $f$ is convex if and only if $\rm{dom}$ $f$ is convex and $f(\mathbf{y}) \ge f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{y}-\mathbf{x} \rangle, \forall \mathbf{x}, \mathbf{y} \in $ ${\rm dom}f$.</p>
<p><strong>Proof:</strong> </p>
<p>$( \Rightarrow )$ </p>
<p>$$<br>\begin{align}<br>f(\mathbf{x} + \theta(\mathbf{y}-\mathbf{x})) &amp;\le f(\mathbf{x}) + \theta [f(\mathbf{y}) - f(\mathbf{x})]\\<br>f(\mathbf{y}) - f(\mathbf{x}) &amp;\ge \lim_{\theta \to 0} \frac{f(\mathbf{x} + \theta(\mathbf{y}-\mathbf{x}))}{\theta}\text{（方向导数）}\\<br>&amp;= \langle \nabla f(\mathbf{x}), \mathbf{y}-\mathbf{x} \rangle<br>\end{align}<br>$$</p>
<p>$ ( \Leftarrow ) $</p>
<p>$$<br>\mathbf{z} = \theta \mathbf{x} + (1 - \theta) \mathbf{y}<br>$$</p>
<p>$$<br> f(\mathbf{x}) \ge f(\mathbf{z}) + \langle \nabla f(\mathbf{z}), \mathbf{x} - \mathbf{z} \rangle \tag{1}<br>$$</p>
<p>$$<br> f(\mathbf{y}) \ge f(\mathbf{z}) + \langle \nabla f(\mathbf{z}), \mathbf{y} - \mathbf{z} \rangle \tag{2}<br>$$</p>
<p>$ \theta (1) + (1 - \theta) (2)  $可得。</p>
<p><strong>Corollary:</strong> Suppose $ f $ is continuously differentiable. Then $ f $ is convex iff  ${\rm dom}f$ is convex and $\langle \nabla f(\mathbf{x})- \nabla f(\mathbf{y}), \mathbf{x}  - \mathbf{y} \rangle \ge 0$.</p>
<p><strong>Theorem 2:</strong> Suppose that $ f $ is continuously differentiable. Then $ f $ is convex iff ${\rm dom}f$  is convex and $ \nabla ^2 f (\mathbf{x}) \ge 0 $.</p>
<p><strong>Proof:</strong> </p>
<p>$ ( \Rightarrow ) $</p>
<p>Let $ \mathbf{x} _ t = \mathbf{x} + t \mathbf{s} $, $t &gt; 0$. Then</p>
<p>$$<br>\begin{align}<br>0 \le \frac{1}{t^2} &amp; \langle \nabla f(\mathbf{x} _ t) - \nabla f(\mathbf{x}) , \mathbf{x} _ t - \mathbf{x} \rangle \\<br>&amp; = \frac {1}{t} \langle \nabla f(\mathbf{x} _ t) - \nabla f(\mathbf{x}), \mathbf{s} \rangle \\<br>&amp; = \frac {1}{t} \int _ 0 ^ t \langle \nabla ^2 f(\mathbf{x} + \tau \mathbf{s})\mathbf{s}, \mathbf{s} \rangle d\tau \text{（微积分基本定理）}\\<br>&amp; \xrightarrow{t \to 0} \langle \nabla ^2 f(\mathbf{x})\mathbf{s}, \mathbf{s} \rangle \\<br>&amp; = \mathbf{s}^T \nabla ^2 f(\mathbf{x}) \mathbf{s}<br>\end{align}<br>$$</p>
<p>$ ( \Leftarrow ) $</p>
<p>$$<br>\begin{align}<br>g(t) &amp; = f(\mathbf{x} + t \mathbf{s}) \\<br>g’(0) &amp;= \langle \nabla f(\mathbf{x}), \mathbf{s} \rangle \\<br>g’’(0) &amp;= \langle \nabla ^2 f(\mathbf{x})\mathbf{s}, \mathbf{s} \rangle \\<br>g(1) &amp; = g(0) + \int _ 0 ^ 1 g’(t) dt \\<br>&amp;  = g(0) + \int _ 0 ^ 1 [g’(0) + \int _ 0 ^ t g’’(\tau) d\tau ] dt \\<br>&amp; \ge g(0) + g’(0) \\<br>f(\mathbf{x} +  \mathbf{s}) &amp; \ge f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{s} \rangle<br>\end{align}<br>$$</p>
<p><strong>Theorem 3:</strong> Suppose $f$ is  continuously differentiable. Then $ \mathbf{x}^  * \in \arg \min \limits _ \mathbf{x}  f ( \mathbf{x})  $  iff $ \nabla f(\mathbf{x}^ *) = 0$, $  f(\mathbf{y}) \ge f(\mathbf{x}) + \langle \nabla f(\mathbf{x}), \mathbf{y} - \mathbf{x} \rangle = f(\mathbf{x})$.</p>
<h4 id="Part-2"><a href="#Part-2" class="headerlink" title="Part 2"></a>Part 2</h4><p>$$<br>\min f(\mathbf{x})<br>$$</p>
<ul>
<li>The objective function $f(\mathbf{x})$ is continuously differentiable.</li>
<li>$f(\mathbf{x})$ is convex.</li>
<li>$\exists {\bf x^ *} \in {\rm dom}f$, s.t. $ f({\bf x^ *})= f^ * = \min f({\bf x}) $.</li>
<li>The gradient of $f$ is Lipschitz continuous, that is, $ \Vert \nabla f({\bf x}) - \nabla f({\bf y}) \Vert \le L \Vert {\bf x}-{\bf y} \Vert$, $L&gt;0$.</li>
</ul>
<h5 id="Algorithm-Gradient-Descent"><a href="#Algorithm-Gradient-Descent" class="headerlink" title="Algorithm:  Gradient Descent"></a>Algorithm:  Gradient Descent</h5><p>$$<br>\begin{align}<br>&amp; \text{Input: An initial point } {\bf x_0} \text{, a constant } \alpha \in (0, \dfrac{2}{L}), \ k = 0 \\<br>&amp; \text{while the termination condition does not hold,  do} \\<br>&amp; ~ ~ ~ ~ ~ ~ ~ ~ k = k + 1 \\<br>&amp; ~ ~ ~ ~ ~ ~ ~ ~ {\bf x_{k+1}}={\bf x_k}-\alpha\nabla f({\bf x_k}) \\<br>&amp; \text{end while}<br>\end{align}<br>$$</p>
<h5 id="Convergence-Rate"><a href="#Convergence-Rate" class="headerlink" title="Convergence Rate"></a>Convergence Rate</h5><p><strong>Definition:</strong> Suppose that the sequence { $a_k$} converges to a number $L$. Then, the sequence is said to converge linearly to $L$ if there exists a number $\mu \in (0, 1)$, s.t. $\lim \limits _ {k \to \infty } \dfrac{\vert a_{k + 1} - L\vert}{\vert a_k - L \vert} = \mu$.</p>
<p><strong>Lemma 1:</strong> Suppose that a function $f \in  C^1$. If $\nabla f$ is Lipschitz continuous with Lipschitz constant $L$, then<br>$$<br>f({\bf y}) \le f({\bf x}) + \langle \nabla f({\bf x}),{\bf y} - {\bf x} \rangle+\dfrac{L}{2} \Vert {\bf y}-{\bf x}\Vert^2.<br>$$</p>
<p><strong>Proof:</strong><br>$$<br>\begin{align}<br>f({\bf y})-f({\bf x}) &amp;= \int^{\bf y}_{\bf x}\nabla f({\bf z}){\bf dz}<br>\\&amp;= \int^1_0 \langle\nabla f({\bf x}+t({\bf y}-{\bf x})),{\bf y}-{\bf x}\rangle dt<br>\\&amp;= \langle\nabla f({\bf x}),{\bf y}-{\bf x}\rangle+\int^1_0 \langle\nabla f({\bf x}+t({\bf y}-{\bf x}))-\nabla f({\bf x}),{\bf y}-{\bf x}\rangle dt<br>\\&amp;\le \langle\nabla f({\bf x}),{\bf y}-{\bf x}\rangle+\int^1_0 \Vert\nabla f({\bf x}+t({\bf y}-{\bf x}))-\nabla f({\bf x})\Vert\Vert{\bf y}-{\bf x}\Vert dt<br>\\&amp;\le \langle\nabla f({\bf x}),{\bf y}-{\bf x}\rangle+L\Vert{\bf y}-{\bf x}\Vert^2\int^1_0 t dt<br>\\&amp;=\langle\nabla f({\bf x}),{\bf y}-{\bf x}\rangle+\frac{L}{2}\Vert {\bf y}-{\bf x}\Vert^2<br>\end{align}<br>$$</p>
<p>（与凹凸性无关）</p>
<p><strong>Lemma 2 （Descent Lemma） :</strong> Suppose that a function $f \in  C^1$. If $\nabla f$ is Lipschitz continuous with Lipschitz constant $L&gt;0$, then $\forall \{ {\bf x_k} \}$ generated by the Gradient Descent Algorithm satisfies<br>$$<br>f({\bf x_{k+1}})\le f({\bf x_k})-{\alpha}(1-\frac{L\alpha}{2})\Vert\nabla f({\bf x_k})\Vert^2.<br>$$</p>
<p>（这也是为什么算法约定$\alpha \in (0, \dfrac{2}{L})$）</p>
<p>下面证明算法可以收敛到最小值，在前提条件下，可以考虑证明：<br>$$<br>\lim_{k \to \infty}\nabla f({\bf x_k}) = \nabla f(\lim_{ k\to \infty}{\bf x_k})=0.<br>$$<br><strong>Proof:</strong></p>
<p>由Lemma 2，<br>$$<br>\begin{align}<br>\Vert \nabla f({\bf x_k})\Vert^2 &amp; \le \frac{f({\bf x_{k}})-f({\bf x_{k+1}})}{\alpha(1-\frac{L\alpha}{2})} \\<br>\sum_k\Vert \nabla  f({\bf x_k})\Vert^2 &amp;\le \frac{f({\bf x_0})-f({\bf x_{k+1}})}{\alpha(1-\frac{L\alpha}{2})}\\<br>&amp;\le\frac{f({\bf x_0})-f^ *}{\alpha(1-\frac{L\alpha}{2})}<br>\end{align}<br>$$<br>这个求和存在固有上界，故<br>$$<br>\lim_{k\to \infty} \nabla f({\bf x_k}) =0<br>$$</p>
<h5 id="Efficiency-and-limitations"><a href="#Efficiency-and-limitations" class="headerlink" title="Efficiency and limitations"></a>Efficiency and limitations</h5><p><strong>Theorem:</strong>  Consider the Problem（$\min f(x)$）and the sequence generated by the Gradient Descent Algorithm. Then the sequence value $f({\bf x_k})$ tends to the optimum function value in a rate of $O(\frac{1}{k})$.</p>
<ol>
<li><p>If $\alpha \in (0, \dfrac{1}{L})$<br>$$<br>f({\bf x_k})-f^ * \le \frac{1}{k}(\frac{1}{2\alpha}\Vert {\bf x_0-x^ *}\Vert^2)<br>$$</p>
</li>
<li><p>If $ \alpha \in (\dfrac{1}{L}, \dfrac{2}{L}) $<br>$$<br>f({\bf x_k})-f^ * \le \frac{1}{k}(\frac{1}{2\alpha}\Vert {\bf x_0-x^ *}\Vert^2+\frac{L\alpha -1}{2-L\alpha}(f({\bf x_0})-f({\bf x ^  * })) )<br>$$</p>
</li>
</ol>
<p><strong>Proof:</strong></p>
<p>As $ {\bf x_{k+1}}={\bf x_k}-\alpha\nabla f({\bf x_k})$ and $ f({\bf y}) \le f({\bf x}) + \langle\nabla f({\bf x}),{\bf y}-{\bf x}\rangle+\dfrac{L}{2}\Vert {\bf y}-{\bf x}\Vert^2 $, </p>
<p>$$<br>f({\bf x_{k+1}})\le f({\bf x_{k}})-(\frac{1}{\alpha}-\frac{L}{2})\Vert{\bf x_{k+1}-x_k}\Vert^2<br>$$</p>
<p>$$<br>f({\bf x_{k+1}})-f^ *\le f({\bf x_{k}})-f^ *-(\frac{1}{\alpha}-\frac{L}{2})\Vert{\bf x_{k+1}-x_k}\Vert^2<br>$$</p>
<p>Consider the convexity of $f$,</p>
<p>$$<br>\begin{align}<br>f({\bf x _ {k + 1} })-f({\bf x^ *}) &amp; \le \langle \nabla f({\bf x_k}), { \bf x_k}-{\bf x^ *}\rangle-(\frac{1}{\alpha}-\frac{L}{2})\Vert{\bf x_{k+1}-x_k}\Vert^2<br>\\ &amp; = -\frac{1}{\alpha} \langle{\bf x_{k+1}}-{\bf x_k},{\bf x_k}-{\bf x^ *}\rangle-(\frac{1}{\alpha}-\frac{L}{2})\Vert{\bf x_{k+1}-x_k}\Vert^2<br>\\ &amp; = -\frac{1}{2\alpha}(\Vert{\bf x_{k+1}}-{\bf x^ *}\Vert^2-\Vert{\bf x_{k+1}}-{\bf x_k}\Vert^2-\Vert{\bf x_{k}}-{\bf x^ *}\Vert^2)-(\frac{1}{\alpha}-\frac{L}{2})\Vert{\bf x_{k+1}}-{\bf x_k}\Vert^2<br>\\ &amp; = \frac{1}{2\alpha}(\Vert{\bf x_{k}}-{\bf x^ *}\Vert^2-\Vert{\bf x_{k+1}}-{\bf x^ *}\Vert^2)-(\frac{1}{2\alpha}-\frac{L}{2})\Vert{\bf x_{k+1}}-{\bf x_k}\Vert^2<br>\end{align}<br>$$</p>
<p>Summing up the inequalities,</p>
<p>$$<br>\begin{align}<br>k(f({\bf x_{k} })-f({\bf x ^ *})) &amp;\le \sum ^ { k-1 } _ { i=0 } ( f({\bf x _ {i + 1}})- f({\bf x^ * }) ) \\<br>&amp; \le \frac{1}{2 \alpha }(\Vert {\bf x_{0} }-{\bf x^ *}\Vert^2-\Vert{\bf x_{k}} - {\bf x ^ *} \Vert^2 ) - (\frac{1}{2\alpha} - \frac{L}{2} ) \sum^{k-1}_{i=0} \Vert{\bf x _ {i+1} }- {\bf x_i} \Vert^2<br>\end{align}<br>$$</p>
<ol>
<li>If $\alpha \in (0, \dfrac{1}{L})$, $\dfrac{1}{2\alpha}-\dfrac{L}{2}&gt;0$, then</li>
</ol>
<p>$$<br>k(f({\bf x_{k}})-f({\bf x^ *})) \le\frac{1}{2\alpha}\Vert{\bf x_{0}}-{\bf x^ *}\Vert^2.<br>$$</p>
<ol start="2">
<li>If $\alpha \in (\dfrac{1}{L}, \dfrac{2}{L})$, $\dfrac{1}{2\alpha}-\dfrac{L}{2}&gt;0$, then</li>
</ol>
<p>$$<br>\begin{align}<br>k (f({\bf x_k })-f({\bf x^ *})) &amp; \le \frac{1}{2 \alpha} (\Vert { \bf x_0} - {\bf x^ *} \Vert ^2 - \Vert{\bf x_k}- {\bf x^ *} \Vert^2) + \frac {L \alpha-1}{2\alpha}\sum^{k-1} _ { i = 0 } \Vert {\bf x _ {i+1} } - {\bf x_i} \Vert^2<br>\\ &amp; \le \frac{1}{2 \alpha} \Vert { \bf x_0} - { \bf x^ * } \Vert^2 + \frac{L \alpha-1}{2 \alpha} \sum^{ \infty} _ {i=0} \Vert {\bf x _ {i+1} } - {\bf x_i} \Vert^2<br>\\ &amp; \le  \frac{1}{2 \alpha} \Vert {\bf x_0} - {\bf x^ *} \Vert^2 + \frac{L \alpha-1}{2 \alpha}\ \frac{2\alpha}{2-L\alpha} (f({\bf x_0})-f({\bf x^ *})) (Lemma \ 2)<br>\end{align}<br>$$</p>
<p>Remark: $\Vert {\bf x_k} - {\bf x^ *} \Vert$ doesn’t always converge to 0.</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><iframe src="/donate/?AliPayQR=/img/AliPayQR.jpg&amp;WeChatQR=/img/WeChatQR.png&amp;GitHub=null&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="http://hytu99.github.io/ml-gradient-descent/" data-id="ck7n3bzcq000aqku7vlqkdr5r" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAMYAAADGCAAAAACs8KCBAAACKElEQVR42u3aMW7DMAwF0Nz/0umaobE/SRutpeepMBxVTwNBiny94uf98Xx7c/w+WfPz/euOBwMD47GM9+Hz7Ztk08crVNf/9g0GBsY+jGQT+daTsHu89XxvGBgYGHm6lsDyHA8DAwPjKsZxID4ucf9RwMXAwHgII186KVDzFPMPanEMDIwHMibp2t1/39LfwMDAeBTjXXyq6WMvESzvCgMDY2lG9SptEiKToxkNamBgYCzKmJSm1xbA+RXeL7/CwMDYgFHdbnXAImkejI4DAwNjG0aeAs7HyE4CaPAeAwNjH0a+aLJcfij5OEUhrcTAwNiG0bsUa97zxe3S6H9hYGAsyqiOWOWDWdUUs9rUPMlwMTAwlmNUQ2TOqH6fj3qctDAxMDAWYvRahnnRmxeo8+IWAwNjVUbSLHwXn7vL2mjYAgMDYyFGL/wlAxOTsrY3XoaBgbE2o7dQ9apu0oTIAzEGBsZ6jCTU9tK1a8NxoYjFwMBYjtFL9aotgSSsV48VAwNjT8ZkVCtZbTJkFmEwMDA2ZtxRpiZHUA3NGBgYOzB6zYA8Ney1OQstTwwMjEUZVw1S9JqayXaPDwUDA2MHxh1L997kIXuStmJgYDyX0RuzyFueeRE7OTIMDIwdGNVmZDJ4kVzoV9PHkzwXAwMD46Iwmh9E+dINAwMD4yLkpC16cWqIgYHxKEZvtDQvPpMAWk1GMTAw9mGMSsdiU7N6SZeviYGBsSjjB4NUS6nnkXJTAAAAAElFTkSuQmCC">分享</a><div class="tags"><a href="/tags/机器学习/">机器学习</a><a href="/tags/学习笔记/">学习笔记</a></div><div class="post-nav"><a class="pre" href="/docker/">Docker入门总结</a><a class="next" href="/ml-linear-regression/">【机器学习笔记】1. Linear Regression</a></div><div id="lv-container" data-id="city" data-uid="MTAyMC80ODk5NS8yNTQ4OQ=="><script>(function(d, s) {
   var j, e = d.getElementsByTagName(s)[0];
   if (typeof LivereTower === 'function') { return; }
   j = d.createElement(s);
   j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
   j.async = true;
   e.parentNode.insertBefore(j, e);
})(document, 'script');
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Paper-Reading/">Paper Reading</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术总结/">技术总结</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/程序设计/">程序设计</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a><span class="category-list-count">2</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/随笔/" style="font-size: 15px;">随笔</a> <a href="/tags/机试/" style="font-size: 15px;">机试</a> <a href="/tags/编程/" style="font-size: 15px;">编程</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/code-search/" style="font-size: 15px;">code search</a> <a href="/tags/保研/" style="font-size: 15px;">保研</a> <a href="/tags/nlp/" style="font-size: 15px;">nlp</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/学习笔记/" style="font-size: 15px;">学习笔记</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/MSRA/" style="font-size: 15px;">MSRA</a> <a href="/tags/hexo/" style="font-size: 15px;">hexo</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/msra-record/">来MSRA的七个月，我都做了些什么？</a></li><li class="post-list-item"><a class="post-list-link" href="/revise-theme/">Maupassant主题增加访问统计</a></li><li class="post-list-item"><a class="post-list-link" href="/ml-svm/">【机器学习笔记】4. Support Vector Machine & Lagrange Duality</a></li><li class="post-list-item"><a class="post-list-link" href="/ml-naive-bayes/">【机器学习笔记】3. Naive Bayes & Logistic Regression</a></li><li class="post-list-item"><a class="post-list-link" href="/docker/">Docker入门总结</a></li><li class="post-list-item"><a class="post-list-link" href="/ml-gradient-descent/">【机器学习笔记】2. Gradient Descent</a></li><li class="post-list-item"><a class="post-list-link" href="/ml-linear-regression/">【机器学习笔记】1. Linear Regression</a></li><li class="post-list-item"><a class="post-list-link" href="/deep-code-search/">【论文阅读】Deep Code Search</a></li><li class="post-list-item"><a class="post-list-link" href="/code-template/">机试模板整理</a></li><li class="post-list-item"><a class="post-list-link" href="/first-article/">终于弄好博客啦</a></li></ul></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">山尤远.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a><span style="font-size:14px;" id="busuanzi_container_site_pv"><br> 本站总访问量  <span rel="nofollow" id="busuanzi_value_site_pv"></span> 人次,</span><span style="font-size:14px;" id="busuanzi_container_site_uv"> 访客数  <span rel="nofollow" id="busuanzi_value_site_uv"></span> 人.</span></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>