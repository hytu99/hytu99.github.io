<!DOCTYPE html><html lang="zh-CN"><head><meta name="generator" content="Hexo 3.9.0"><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="水远，怎知流水外，却是乱山尤远。"><title>【机器学习笔记】4. Support Vector Machine &amp; Lagrange Duality | 山尤远</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/8.0.1/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/pure-min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/1.0.0/grids-responsive-min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/3.4.0/jquery.min.js"></script><link rel="icon" mask sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">【机器学习笔记】4. Support Vector Machine &amp; Lagrange Duality</h1><a id="logo" href="/.">山尤远</a><p class="description">THY’s Blog</p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a><a href="/atom.xml"><i class="fa fa-rss"> 订阅</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">【机器学习笔记】4. Support Vector Machine &amp; Lagrange Duality</h1><div class="post-meta">Dec 2, 2019<span> | </span><span class="category"><a href="/categories/机器学习/">机器学习</a></span><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> 阅读</span></span><span class="post-time"><span class="post-meta-item-text"> | </span><span class="post-meta-item-icon"><i class="fa fa-keyboard-o"></i><span class="post-count"> 2.4k</span><span class="post-meta-item-text"> 字</span></span></span><span class="post-time"> | <span class="post-meta-item-icon"><i class="fa fa-hourglass-half"></i><span class="post-count"> 14</span><span class="post-meta-item-text"> 分钟</span></span></span></div><a class="disqus-comment-count" data-disqus-identifier="ml-svm/" href="/ml-svm/#disqus_thread"></a><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">文章目录</div><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Support-Vector-Machine"><span class="toc-text">Support Vector Machine</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#SVM-for-linear-separable-data"><span class="toc-text">SVM for linear separable data</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Maximum-Margin-Classifier"><span class="toc-text">Maximum Margin Classifier</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Not-separable"><span class="toc-text">Not separable</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lagrange-Duality"><span class="toc-text">Lagrange Duality</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SVM-amp-SVM-Dual"><span class="toc-text">SVM &amp; SVM Dual</span></a></li></ol></div></div><div class="post-content"><blockquote>
<p>整理自同学的笔记</p>
</blockquote>
<h3 id="Support-Vector-Machine"><a href="#Support-Vector-Machine" class="headerlink" title="Support Vector Machine"></a>Support Vector Machine</h3><p>Training data: $ \{ (\mathbf{\mathbf{x}}_i, y ) \} _ {i=1} ^ n$,  $y_i \in \mathcal{C} = \{ -1, 1 \}$.</p>
<p>Aim: $f(\mathbf{\mathbf{x}}, \mathbf{w}, b) = b + \sum_{j=1}^d w_j x_j$, s.t. $y_i = \text{sign}(f(\mathbf{\mathbf{x}}_i, w, b))$</p>
<h4 id="SVM-for-linear-separable-data"><a href="#SVM-for-linear-separable-data" class="headerlink" title="SVM for linear separable data"></a>SVM for linear separable data</h4><p><strong>Definition:</strong> A training sample is linear separate if there exists $(\hat{\mathbf{w}}, \hat{b})$, s.t. $y_i = \text{sign}  (f(\mathbf{\mathbf{x}}_i, \hat{\mathbf{w}}, \hat{b}))$, $\forall i \in [n] = \{1, 2, \cdots, n \}$, which is equivalent to $y_i f(\mathbf{\mathbf{x}}_i, \hat{\mathbf{w}}, \hat{b}) &gt; 0$, $\forall i \in [n]$.</p>
<img src="https://pic3.zhimg.com/v2-197913c461c1953c30b804b4a7eddfcc_1200x500.jpg" width="400" height="400" alt="SVM示意图" align="center">

<p>点$\mathbf{\mathbf{x}}_i$到线$ \langle {\bf w}, \mathbf{\mathbf{x}} \rangle + b = 0$的距离$d(\mathbf{\mathbf{x}}_i;\mathbf{w},b) = \dfrac{y_i(\langle \mathbf{w}, \mathbf{\mathbf{x}}_i \rangle + b)}{\Vert \mathbf{w} \Vert_2}$.</p>
<p>$$<br>\max \limits _ {\mathbf{w}, b} \min \limits _ {\mathbf{\mathbf{x}} _ i \in D} margin(\mathbf{w}, b, D) = \max \limits_{\mathbf{w}, b} \min \limits _ {\mathbf{\mathbf{x}} _ i \in D } d(\mathbf{\mathbf{x}} _ i) = \max \limits _ {\mathbf{w}, b} \min \limits _ {\mathbf{\mathbf{x}} _ i \in D}  \dfrac{y_i( \langle\mathbf{w}, \mathbf{\mathbf{x}} _ i \rangle+ b)}{\Vert \mathbf{w} \Vert _ 2}<br>$$</p>
<p><strong>Assumption 1:</strong> Training sample $D = \{ (\mathbf{\mathbf{x}}_i, y_i) \} $, is linear separable.</p>
<p><strong>Definition:</strong> </p>
<p>The geometric margin $\gamma_f (\mathbf{z})$ of a linear classifier $f({\bf \mathbf{x}},{\bf w}, b) = \langle {\bf w}, {\bf \mathbf{x}} \rangle + b$ at a point $\mathbf{z}$ is its sigmoid Euclidean Distance to the hyperplane $ \{ \mathbf{\mathbf{x}}  :   \langle {\bf w}, {\bf \mathbf{x}} \rangle + b = 0\}$.<br>$$<br>\gamma_f (\mathbf{z})= \dfrac{y_i(\langle \mathbf{w}, \mathbf{z}_i \rangle + b)}{\Vert \mathbf{w} \Vert_2}<br>$$</p>
<p>The geometric margin $\gamma_f$ of a linear classifier $f$ for sample $S = \{ {\bf \mathbf{x}}_1, \cdots, {\bf x_n} \}$ is the minimum margin over the points in the sample. </p>
<p>$$<br>\gamma_f = \min \limits_{i \in [n]} \gamma_f ({\bf \mathbf{x}}_i)<br>$$</p>
<h4 id="Maximum-Margin-Classifier"><a href="#Maximum-Margin-Classifier" class="headerlink" title="Maximum Margin Classifier"></a>Maximum Margin Classifier</h4><p>$$<br>\max \limits_ { {\bf w}, b} \gamma_f = \max \limits_ { {\bf w}, b} \left \{ \frac{1}{\Vert \mathbf{w} \Vert} \min \limits_{i \in [n] } y _ i( \langle \mathbf{w}, \mathbf{\mathbf{x}}_i \rangle + b) \right \}<br>$$</p>
<p>即</p>
<p>$$<br>\max \limits _ { {\bf w}, b} \frac{1}{\Vert {\bf w} \Vert},  \\<br>\text{s.t. } \min \limits _ {i \in [n]} y _ i( \langle \mathbf{w}, \mathbf{\mathbf{x}} _ i \rangle + b) = 1  \\<br>\Rightarrow y _ i( \langle \mathbf{w} , \mathbf{\mathbf{x}} _ i \rangle + b) \ge 1 \\<br>\Rightarrow \min \limits_{ {\bf w}, b} \frac{1}{2} \Vert {\bf w} \Vert ^ 2<br>$$</p>
<p>用反证法可证等号可以取到。</p>
<p><strong>Definition:</strong> Given a SVM classifier $\langle \mathbf{w}, \mathbf{\mathbf{x}}_i \rangle + b = 0$, the marginal hyperplanes are determined by $\vert \langle \mathbf{w}, \mathbf{\mathbf{x}}_i \rangle + b \vert  = 1$. The support vectors are the data instance on the marginal hyperplanes. （ i.e. $ \{  {\bf \mathbf{x}}_i  : | \langle \mathbf{w}, \mathbf{\mathbf{x}}_i \rangle + b \vert  = 1 , {\bf \mathbf{x}}_i \in S \} $ ）</p>
<h4 id="Not-separable"><a href="#Not-separable" class="headerlink" title="Not separable"></a>Not separable</h4><p>minimize $\frac{1}{2} \Vert {\bf w} \Vert ^ 2 + C(training \ errors)$</p>
<p>minimize  $\frac{1}{2} \Vert {\bf w} \Vert ^ 2 + C(distance \ of\ the \ error \  points \ and \ its \ correct \ position)$</p>
<p>SVM for non-separate cases:</p>
<p>$$<br>\min \limits _ { {\bf w}, b, \epsilon} \frac{1}{2} \Vert \mathbf{w} \Vert + C \sum _ {i=1} ^ {n} \epsilon_i, \\<br>\text{s.t. } y_i (\langle \mathbf{w}, \mathbf{\mathbf{x}}_i \rangle + b) \ge 1 - \epsilon_i, i \in [n] \\  \epsilon_i \ge 0, i \in [n]<br>$$</p>
<h3 id="Lagrange-Duality"><a href="#Lagrange-Duality" class="headerlink" title="Lagrange Duality"></a>Lagrange Duality</h3><p>Consider the problem:</p>
<p>$$<br>\begin{align}<br>\min f(\mathbf{x}) \tag{1} \\<br>\text{s.t. } g_i(\mathbf{x}) &amp; \le 0, i = 1,\cdots,m \\<br>h_i(\mathbf{x}) &amp; = 0, i = 1,\cdots, p \\<br>\mathbf{x} &amp; \in X<br>\end{align}<br>$$</p>
<p>$f$, $g_i$, $h_i$ are all continously differentiable.</p>
<p>$$<br>g(\mathbf{x}) = \left [<br>\begin{matrix}<br>g_1(\mathbf{x}) \\<br>\vdots \\<br>g_m(\mathbf{x})<br>\end{matrix}<br>\right],<br>h(\mathbf{x}) = \left [<br>\begin{matrix}<br>h_1(\mathbf{x}) \\<br>\vdots \\<br>h_p(\mathbf{x})<br>\end{matrix}<br>\right]<br>$$</p>
<p>Feasible Set: $D = \{  \mathbf{x}  :  g(\mathbf{x}) \le 0 , h(\mathbf{x}) = 0, \mathbf{x} \in X \}$.</p>
<p>Each $\mathbf{x} \in D$ is called a feasible solution. The optimal function value is $f^* = \inf \limits _ { \mathbf{x} \in D } f(\mathbf{x})$.</p>
<hr>
<p>Transition from the domain to the image $S = \{ (g(\mathbf{x}), h(\mathbf{x}), f(\mathbf{x}))  :  \mathbf{x} \in X \}$ （$\dim =m + p + 1 $）</p>
<p><strong>Definition 1:</strong> Associated with the primal problem, we define the Lagrangian $L$: $\mathbb{R}^n \times \mathbb{R} ^ m \times \mathbb{R} ^ p \rightarrow \mathbb{R}$.<br>$$<br>L(\mathbf{\mathbf{x}}, \lambda, \mu) = f(\mathbf{\mathbf{x}}) + \sum _ {i = 1} ^ m \lambda_i g_i(\mathbf{\mathbf{x}}) + \sum _ {i = 1} ^ p \mu_i h_i(\mathbf{\mathbf{x}})<br>$$</p>
<p><strong>Definition 2:</strong> A vector $ ( \lambda ^ *, \mu ^ *) = (\lambda _ 1 ^ *, \cdots, \lambda_m ^ *, \mu_1 ^ *, \cdots, \mu_p ^ *)$  is said to be a geometric multiplier vector（or simply geometric multiplier）for the primal problem if:<br>$$<br>\lambda_i ^ * \ge 0, i = 1, \cdots, m \text{ and } f ^ * = \inf _ {\mathbf{x} \in X} L(\mathbf{\mathbf{x}}, \lambda ^ *, \mu ^ *)<br>$$</p>
<p><strong>Lemma（Visualization Lemma）:</strong> </p>
<ol>
<li>The hyperplane with normal $(\lambda, \mu, 1)$ that pass through $(g(\mathbf{\mathbf{x}}), h(\mathbf{\mathbf{x}}), f(\mathbf{\mathbf{x}}))$ intercepts the vertical axis $\{ (\mathbf{0}, z), z \in \mathbb{R} \}$ at the level $L(\mathbf{\mathbf{x}}, \lambda, \mu)$.</li>
<li>Among all hyperplanes with normal $(\lambda, \mu, 1)$ that contains in their positive half space the set $S$, the highest attained level of interception of the vertical axis is $\inf \limits_ {\mathbf{x} \in X} L(\mathbf{\mathbf{x}}, \lambda, \mu)$.</li>
</ol>
<p><strong>Proposition:</strong> Let $(\lambda ^ *, \mu ^ *)$ be a geometric multiplier. Then $\mathbf{\mathbf{x}} ^ *$ is a global minimum of the primal problem iff $\mathbf{\mathbf{x}} ^ * \in \arg \min \limits _ {\mathbf{\mathbf{x}} \in X} L(\mathbf{\mathbf{x}}, \lambda ^ *, \mu ^ *)$, $ \lambda _ i ^ * g _ i(\mathbf{\mathbf{x}} ^ *) = 0$, $  i = 1, \cdots, m$（complementary slackness）.</p>
<p><strong>Proof:</strong></p>
<p>（$\Rightarrow$）</p>
<p>Suppose $\mathbf{x} ^ *$ is a global minimum. Then $\mathbf{x} ^ *$ must be feasible, and thus</p>
<p>$$<br>f(\mathbf{x} ^ *) \ge L(\mathbf{x} ^ *, \lambda ^ *, \mu ^ *) \ge f ^ * = f(\mathbf{x} ^ *) + \sum _ {i = 1} ^ m \lambda_i g _ i(\mathbf{x} ^ *) + \sum _ {i = 1} ^ p \mu_i h _ i(\mathbf{x} ^ *)<br>$$</p>
<p>The definition of $ f  ^ * $ leads to $ f ^ * = f(\mathbf{x} ^ * )  $, which implies that </p>
<p>$$<br>f ( \mathbf{x} ^ * ) = L (\mathbf{x} ^ * ) = f ^ *  = \inf \limits  _ {\mathbf{x} \in \mathcal{X} } L (\mathbf{x}, \lambda ^ *, \mu ^ *) \\<br>\Rightarrow \mathbf{x} ^ * = \arg \min \limits _ {\mathbf{x} \in X} L(\mathbf{x}, \lambda ^ *, \mu ^ *) \text{ and } f ( \mathbf{x} ^ *) = L(\mathbf{x} ^ *) = f(\mathbf{x} ^ *) + \sum _ {i = 1} ^ m \lambda_i g _ i(\mathbf{x} ^ *) + \sum _ {i = 1} ^ p \mu_i h _ i(\mathbf{x} ^ *) \\<br>\Rightarrow \lambda _ i ^ * g _ i(\mathbf{x} ^ *) = 0<br>$$</p>
<p>（$\Leftarrow$）<br>$$<br>f (\mathbf{x} ^ * ) = L(\mathbf{x} ^ *, \lambda ^ *, \mu ^ *) \le L(\mathbf{x} , \lambda ^ *, \mu ^ *) =  f(\mathbf{x}) + \sum _ {i = 1} ^ m \lambda_i g _ i(\mathbf{x} ^ *) + \sum _ {i = 1} ^ p \mu_i h _ i(\mathbf{x} ^ *) \le f(\mathbf{x})<br>$$</p>
<p><strong>Lagrange Duality:</strong></p>
<p>Lagrange Dual Function: $ q(\lambda, \mu) = \inf \limits_ {\mathbf{x} \in X} L (\mathbf{x}, \lambda, \mu)$.</p>
<p>Lagrange Dual Problem: $\max q(\lambda, \mu)$, s.t. $\lambda \ge 0$.</p>
<p>Dual optimal value: $q ^ * = \sup \limits _ { \{ (\lambda, \mu)  :  \lambda \ge 0 \} } q (\lambda, \mu)$</p>
<p>$\text{dom } q = \{ (\lambda, \mu)  :  q(\lambda, \mu) &gt; - \infty \}$</p>
<p><strong>convex:</strong></p>
<ol>
<li>$\text{dom } q \cap \{ (\lambda, \mu)  :  \lambda \ge 0 \} $ is convex.</li>
<li>$-q$ is convex. （$f(\mathbf{x}) = \sup \limits_ {y \in \cal{Y}}l(\mathbf{x},y)$, $l(\mathbf{x},y)$ is convex $\Rightarrow f(\mathbf{x})$ is convex）</li>
</ol>
<p><strong>Theorem（Week Duality Theorem）:</strong> $q ^ * \le f ^ *$</p>
<p><strong>Proof:</strong> $\forall (\lambda, \mu), q(\lambda, \mu) = \inf \limits _ {\mathbf{x} \in X} L(\mathbf{x}, \lambda, \mu) \le \inf \limits _ {\mathbf{x} \in D} L(\mathbf{x}, \lambda, \mu) \le f ^ *$</p>
<hr>
<p><strong>Definition:</strong> Consider $f: X \rightarrow Y$</p>
<ol>
<li><p>The value $f(x) \in Y$ that it assumes at element $ x  \in X$ is called the image of $x$.</p>
</li>
<li><p>The image of a set $A \subset X$ under the mapping $f$ is $f(A) = \{ y \in Y  :  \exists x \in A, \text{s.t. } f(x) = y  \}$.</p>
</li>
<li><p>The preimage of as set $B \subset Y$ is $f ^ {-1} (B) := \{ x \in X : f(x) \in B \}$</p>
<p>eg: $f(X) = \det (A)$, $f(x ^ 2) = 2 x $ .</p>
</li>
</ol>
<p><strong>Definition:</strong> A hyperplane $H$ in $ \mathbb{R} ^ {d +1}$ is specified by a linear equation involving a nonzero vector $(\mathbf{u}, u_0)$ （called the normal vector of $H$）, where $\mathbf{u} \in  \mathbb{R} ^ d$ and $u_0 \in \mathbb{R}$ and by a constraint $C$ as follows:<br>$$<br>H = \{ ({\bf w}, z) : {\bf w} \in \mathbb{R} ^ d, z \in \mathbb{R}, u_0 z + \langle \mathbf{u}, \mathbf{w} \rangle = C \}<br>$$</p>
<p>Hyperplane defines two half-spaces: the positive half-space $H ^ + = \{ ({\bf w}, z) : {\bf w} \in \mathbb{R} ^ d, z \in \mathbb{R}, u_0 z + \langle \mathbf{u}, \mathbf{w} \rangle \ge  C \}$ and the negative half-space $H ^ + = \{ ({\bf w}, z) : {\bf w} \in \mathbb{R} ^ d, z \in \mathbb{R}, u_0 z + \langle \mathbf{u}, \mathbf{w} \rangle \le  C \}$.</p>
<p>$$<br>l({\bf w}, z) = u_0 z +  \langle \mathbf{u}, \mathbf{w} \rangle  \\<br>$$</p>
<p><img src="pic2" alt></p>
<p><strong>Definition:</strong> Duality gap is $f ^ * - q ^ *$.</p>
<p><strong>Proposition:</strong> </p>
<ol>
<li><p>If there is no duality gap, the set of geometric multipliers is equal to the set of optimal dual solution.</p>
</li>
<li><p>If there is duality gap, the set of geometric multipliers is empty.</p>
</li>
</ol>
<p><strong>Optimality conditions:</strong> </p>
<p>A pair $\mathbf{\mathbf{x}} ^ * $ and $  ( \lambda ^ * , \mu ^ * ) $ is an optimal solution and geometric multiplier iff </p>
<p>$$<br>\mathbf{x} ^ * \in X, g(\mathbf{x} ^ *) \le 0, h(\mathbf{x} ^ * ) = 0. \text{(Primal Feasibility)} \\<br>\lambda ^ * \ge 0  \text{(Dual Feasibility)} \\<br>\mathbf{x} ^ * \in \arg \min \limits_ {\mathbf{x} \in X} L(\mathbf{x}, \lambda ^ *, \mu ^ *) \text{(Lagrangian Optimality )} \\<br>\lambda _ i ^ * g _ i ^ * (\mathbf{x}) = 0, i = 1,\cdots,m \text{(Complementary Slackness)}<br>$$</p>
<p><strong>Saddle Point Theorem:</strong></p>
<p>A pair $\mathbf{\mathbf{x}} ^ * $ and $  ( \lambda ^ * , \mu ^ * ) $ is an optimal solution and geometric multiplier iff $\mathbf{\mathbf{x}} ^ * \in X$, $\lambda ^ * \ge 0$ and $(\mathbf{\mathbf{x}} ^ *, \lambda ^ *, \mu ^ * ) $ is a saddle point of the Lagrangian. i.e.</p>
<p>$$<br>L((\mathbf{\mathbf{x}} ^ *, \lambda, \mu) \le L(\mathbf{\mathbf{x}} ^ *, \lambda ^ *, \mu ^ * ) \le (\mathbf{\mathbf{x}}, \lambda ^ *, \mu ^ * ) ), \forall \mathbf{\mathbf{x}} \in X, \lambda \ge 0<br>$$</p>
<p><strong>Strong Duality Theorem:</strong></p>
<p>Consider the primal problem. Suppose that $f$ is convex , $X$ is a polyhedral, i.e. $X = \{ {\bf \mathbf{x}}:  \langle {\bf a} _ i, {\bf \mathbf{x}} \rangle  \le b, i = 1, \cdots, r \} $, $g_i$ and $h_i$ are linear and $f ^ * $ is finite. Then there is no duality gap and there exists at least one geometric multiplier （primal and dual problems have optimal solutions）.</p>
<h3 id="SVM-amp-SVM-Dual"><a href="#SVM-amp-SVM-Dual" class="headerlink" title="SVM &amp; SVM Dual"></a>SVM &amp; SVM Dual</h3><p><strong>SVM:</strong></p>
<p>$$<br>\min _ {\mathbf{w}, b} \frac{1}{2} \Vert {\bf w} \Vert ^ 2 + C \sum _ {i = 1} ^ n \epsilon_i \\<br>\text{s.t. } y_i ( \langle \mathbf{w}, \mathbf{\mathbf{x}}_i \rangle + b) \ge 1 - \epsilon_i, i = 1, \cdots, n \\<br>\epsilon_i \ge 0, i = 1, \cdots, n<br>$$</p>
<p>$$<br>L(\mathbf{w}, b, \epsilon, \alpha, u) = \frac{1}{2} \Vert \mathbf{w} \Vert ^ 2 + C \sum _ {i = 1} ^ n \epsilon_i + \sum _ {i = 1} ^ n \alpha _ i (1 - \epsilon _ i - y_i ( \langle \mathbf{w}, x_i \rangle + b) ) - \sum _ {i = 1} ^ n u_i \epsilon_i, \alpha \ge 0, u \ge 0<br>$$</p>
<p>$$<br>\begin{align}<br>q(a, u)  = &amp; \inf _ {\mathbf{w}, b, \epsilon} L(b, \epsilon, \alpha, u) \\<br>= &amp; \inf _ {\mathbf{w}} \frac{1}{2} \Vert {\bf w} \Vert ^ 2 - \sum _ {i = 1} ^ n \alpha _ i  y_i \langle \mathbf{w},  \mathbf{\mathbf{x}}_i \rangle  \\ &amp; + \inf _ {b}  b  \sum _ {i = 1} ^ n  \alpha _ i y _ i \\ &amp; + \inf _ {\epsilon} \sum _ {i = 1} ^ n (C - \alpha _ i - u _ i) \epsilon_i<br>\end{align}<br>$$</p>
<p>$$<br>\begin{align}<br>\left. \nabla_\mathbf{w} L(\mathbf{w}, b, \epsilon, \alpha, u)  \right| _ {\mathbf{w} = \mathbf{\hat{w}}} = 0 &amp; \Rightarrow \mathbf{\hat{w}} - \sum _ {i = 1} ^ n \alpha _ i y _i \mathbf{\mathbf{x}} _ i - 0 \\<br>\left. \nabla_b L(\mathbf{w}, b, \epsilon, \alpha, u)  \right| _ {b = \hat{b}} = 0 &amp; \Rightarrow - \sum _ {i = 1} ^ n \alpha_i y _i = 0 \\<br>\left. \nabla_\epsilon L(\mathbf{w}, b, \epsilon, \alpha, u)  \right| _ {\epsilon = \hat{\epsilon}} = 0 &amp; \Rightarrow C - \alpha_i - u _ i = 0<br>\end{align}<br>$$</p>
<p>$$<br>\max q(\alpha, u) = - \frac{1}{2} \sum _ {i =1} ^ n \sum _ {j = 1} ^ n \alpha_i \alpha_j y_i y _ j \langle {\bf \mathbf{x}} _ i, {\bf \mathbf{x}} _ j \rangle + \sum _ {i = 1} ^ n \alpha _ i \\<br>\text{s.t. }  \sum _ {i = 1} ^ n \alpha_i y _i = 0 , \alpha _ i \ge 0 \\<br> C - \alpha_i - u _ i = 0, u_i \ge 0<br>$$</p>
<p><strong>SVM Dual:</strong></p>
<p>$$<br>\max q(\alpha)<br>\text{s.t. } \sum _ {i = 1} ^ n \alpha _ i y _ i = 0 \\<br>\alpha _ i \in [0, C], i = 1, \cdots, n<br>$$</p>
<p><strong>Proposition:</strong></p>
<p>Let $\alpha ^ * $ be one of the dual optimal solutions.</p>
<p>$$<br>\mathbf{w} ^ * = \sum _ {i = 1} ^ n \alpha _ i ^ * y _ i \mathbf{\mathbf{x}} _ i \\<br> \alpha _ i (1 - \epsilon _ i - y_i ( \langle \mathbf{w}, \mathbf{\mathbf{x}}_i \rangle + b) ) = 0, \forall i \text{(Complementary Slackness)}<br>$$</p>
<p>$ \alpha _ k ^ * $ is one of the entries of $\alpha ^ *$ and $ \alpha _ k ^ * \in (0, C) $, then:</p>
<p>$$<br>(1 - \epsilon _ i - y_i ( \langle \mathbf{w}, x_i \rangle + b) ) = 0 \\<br>\alpha _ k ^ * \in (0, C) \Rightarrow u_k ^ * \in (0, C) \Rightarrow \epsilon _ k ^ * = 0 \\<br>b ^ * = y _ k - \langle {\bf w} ^ *, {\bf \mathbf{x} _ k} \rangle<br>$$</p>
<script>
        document.querySelectorAll('.github-emoji')
          .forEach(el => {
            if (!el.dataset.src) { return; }
            const img = document.createElement('img');
            img.style = 'display:none !important;';
            img.src = el.dataset.src;
            img.addEventListener('error', () => {
              img.remove();
              el.style.color = 'inherit';
              el.style.backgroundImage = 'none';
              el.style.background = 'none';
            });
            img.addEventListener('load', () => {
              img.remove();
            });
            document.body.appendChild(img);
          });
      </script></div><iframe src="/donate/?AliPayQR=/img/AliPayQR.jpg&amp;WeChatQR=/img/WeChatQR.png&amp;GitHub=null&amp;BTCQR=null&amp;BTCKEY=null&amp;PayPal=null" style="overflow-x:hidden; overflow-y:hidden; border:0xp none #fff; min-height:240px; width:100%;" frameborder="0" scrolling="no"></iframe><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a class="article-share-link" data-url="http://hytu99.github.io/ml-svm/" data-id="ck7ifmgh7000f7gu70u2to70u" data-qrcode="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAK4AAACuCAAAAACKZ2kyAAABzElEQVR42u3aQW4CMRAEQP7/aSLlRA4s3eO1haLyCSEWyhxGnh4/HvF6/q7X16/vXD/17tnkG4YLFxd3mfu8XMlnrp+63nZuwMXFPc9NildSyNbpH2y4uLhfxn0HTQ4u1xvGxcX9H9xrdLttXFzc7+TO2piEmB+Abu7VcHFxF7izwPTe19vzXVxc3JumEm08mh+GhiEsLi7uEe4s/mjbleRYU8QluLi4B7ntz+eNTbLVou3BxcU9wp2Vqh2bjAJTXFzczdxrexJrtiOZfOD6tlfDxcXdzM0rXI6eHVnqUQouLu4Rbn7louW25awYseDi4m7grhxT8kg0/yM+NEW4uLhHuLOjySz6nF32KrJeXFzcm7izgehsA3ngUtddXFzcDdyV2Ut7YWvWy+Di4p7kJuFIcvS5F7d0CQMXF3eZe0MwUV6hmA1g/jQ/uLi4m7ntgSaHrk91kitiuLi4O7jPcuX7zp8tWi9cXNwj3LuSh/addgSLi4t7npsXr7yErac19RwYFxd3AzePM/Joo41XitgUFxf3y7jr5W9pDIOLi/tl3CQWactfXeZwcXEPcvOSlBPzcUsRjuDi4h7hzkaeyY/lY5W2xcLFxd3G/QG9R91EsZDmbwAAAABJRU5ErkJggg==">分享</a><div class="tags"><a href="/tags/机器学习/">机器学习</a><a href="/tags/学习笔记/">学习笔记</a></div><div class="post-nav"><a class="next" href="/ml-naive-bayes/">【机器学习笔记】3. Naive Bayes &amp; Logistic Regression</a></div><div id="disqus_thread"><div class="btn_click_load"><button class="disqus_click_btn">阅读评论（请确保 Disqus 可以正常加载）</button></div><script type="text/javascript">var disqus_config = function () {
    this.page.url = 'http://hytu99.github.io/ml-svm/';
    this.page.identifier = 'ml-svm/';
    this.page.title = '【机器学习笔记】4. Support Vector Machine &amp; Lagrange Duality';
  };</script><script type="text/javascript" id="disqus-lazy-load-script">$.ajax({
url: 'https://disqus.com/next/config.json',
timeout: 2500,
type: 'GET',
success: function(){
  var d = document;
  var s = d.createElement('script');
  s.src = '//ustcthy.disqus.com/embed.js';
  s.setAttribute('data-timestamp', + new Date());
  (d.head || d.body).appendChild(s);
  $('.disqus_click_btn').css('display', 'none');
},
error: function() {
  $('.disqus_click_btn').css('display', 'block');
}
});</script><script type="text/javascript" id="disqus-click-load">$('.btn_click_load').click(() => {  //click to load comments
    (() => { // DON'T EDIT BELOW THIS LINE
        var d = document;
        var s = d.createElement('script');
        s.src = '//ustcthy.disqus.com/embed.js';
        s.setAttribute('data-timestamp', + new Date());
        (d.head || d.body).appendChild(s);
    })();
    $('.disqus_click_btn').css('display','none');
});</script><script type="text/javascript" id="disqus-count-script">$(function() {
     var xhr = new XMLHttpRequest();
     xhr.open('GET', '//disqus.com/next/config.json', true);
     xhr.timeout = 2500;
     xhr.onreadystatechange = function () {
       if (xhr.readyState === 4 && xhr.status === 200) {
         $('.post-meta .post-comments-count').show();
         var s = document.createElement('script');
         s.id = 'dsq-count-scr';
         s.src = 'https://ustcthy.disqus.com/count.js';
         s.async = true;
         (document.head || document.body).appendChild(s);
       }
     };
     xhr.ontimeout = function () { xhr.abort(); };
     xhr.send(null);
   });
</script></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><div class="search-form"><input id="local-search-input" placeholder="Search" type="text" name="q" results="0"><div id="local-search-result"></div></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Paper-Reading/">Paper Reading</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/技术总结/">技术总结</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/机器学习/">机器学习</a><span class="category-list-count">4</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/程序设计/">程序设计</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/随笔/">随笔</a><span class="category-list-count">1</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> 标签</i></div><div class="tagcloud"><a href="/tags/编程/" style="font-size: 15px;">编程</a> <a href="/tags/随笔/" style="font-size: 15px;">随笔</a> <a href="/tags/学习笔记/" style="font-size: 15px;">学习笔记</a> <a href="/tags/保研/" style="font-size: 15px;">保研</a> <a href="/tags/机试/" style="font-size: 15px;">机试</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/算法/" style="font-size: 15px;">算法</a> <a href="/tags/docker/" style="font-size: 15px;">docker</a> <a href="/tags/code-search/" style="font-size: 15px;">code search</a> <a href="/tags/deep-learning/" style="font-size: 15px;">deep learning</a> <a href="/tags/nlp/" style="font-size: 15px;">nlp</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> 最近文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/ml-svm/">【机器学习笔记】4. Support Vector Machine & Lagrange Duality</a></li><li class="post-list-item"><a class="post-list-link" href="/ml-naive-bayes/">【机器学习笔记】3. Naive Bayes & Logistic Regression</a></li><li class="post-list-item"><a class="post-list-link" href="/docker/">Docker入门总结</a></li><li class="post-list-item"><a class="post-list-link" href="/ml-gradient-descent/">【机器学习笔记】2. Gradient Descent</a></li><li class="post-list-item"><a class="post-list-link" href="/ml-linear-regression/">【机器学习笔记】1. Linear Regression</a></li><li class="post-list-item"><a class="post-list-link" href="/deep-code-search/">【论文阅读】Deep Code Search</a></li><li class="post-list-item"><a class="post-list-link" href="/code-template/">机试模板整理</a></li><li class="post-list-item"><a class="post-list-link" href="/first-article/">终于弄好博客啦</a></li><li class="post-list-item"><a class="post-list-link" href="/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-comment-o"> 最近评论</i></div><script type="text/javascript" src="//ustcthy.disqus.com/recent_comments_widget.js?num_items=5&amp;hide_avatars=1&amp;avatar_size=32&amp;excerpt_length=20&amp;hide_mods=1"></script></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2020 <a href="/." rel="nofollow">山尤远.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a><span style="font-size:14px;" id="busuanzi_container_site_pv"><br> 本站总访问量  <span rel="nofollow" id="busuanzi_value_site_pv"></span> 人次,</span><span style="font-size:14px;" id="busuanzi_container_site_uv"> 访客数  <span rel="nofollow" style="font-size:14px;" id="busuanzi_value_site_uv"></span> 人.</span></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css"><script type="text/javascript" src="/js/search.js?v=0.0.0"></script><script>var search_path = 'search.xml';
if (search_path.length == 0) {
   search_path = 'search.xml';
}
var path = '/' + search_path;
searchFunc(path, 'local-search-input', 'local-search-result');
</script><script src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>